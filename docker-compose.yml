services:
  # ======================
  # Postgres compartido
  # ======================
  db:
    image: postgres:15
    container_name: news_nlp_db
    restart: unless-stopped
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: postgres
    ports:
      # Host 5433 -> contenedor 5432
      - "5433:5432"
    volumes:
      - news_nlp_db_data:/var/lib/postgresql/data
      - ./docker/postgres-init.sql:/docker-entrypoint-initdb.d/postgres-init.sql:ro

  # ======================
  # API FastAPI
  # ======================
  api:
    build:
      context: .
      dockerfile: app.Dockerfile
    container_name: news_nlp_api
    restart: unless-stopped
    env_file:
      - .env
    depends_on:
      - db
    ports:
      # Host 8000 -> contenedor 8000
      - "8000:8000"
    command: >
      uvicorn news_nlp.api.app:app
        --host 0.0.0.0
        --port 8000

  # ======================
  # MLflow server
  # ======================
  mlflow:
    build:
      context: .
      dockerfile: app.Dockerfile
    container_name: news_nlp_mlflow
    restart: unless-stopped
    env_file:
      - .env
    depends_on:
      - db
    ports:
      # Host 5000 -> contenedor 5000
      - "5000:5000"
    volumes:
      - mlflow_artifacts:/mlflow/artifacts
    command: >
      mlflow server
        --backend-store-uri ${MLFLOW_BACKEND_STORE_URI}
        --artifacts-destination ${MLFLOW_ARTIFACT_ROOT}
        --host 0.0.0.0
        --port 5000

  # ======================
  # Airflow init (one-shot)
  # ======================
  airflow-init:
    image: apache/airflow:2.9.0-python3.10
    container_name: news_nlp_airflow_init
    restart: "no"
    env_file:
      - .env
    environment:
      # Necesario para usar Postgres desde Airflow
      _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS}
    depends_on:
      - db
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    command: >
      bash -c "
        airflow db migrate &&
        airflow users create
          --username admin
          --password admin
          --firstname Admin
          --lastname User
          --role Admin
          --email admin@example.com
      "

  # ======================
  # Airflow webserver
  # ======================
  airflow-webserver:
    image: apache/airflow:2.9.0-python3.10
    container_name: news_nlp_airflow_webserver
    restart: unless-stopped
    env_file:
      - .env
    environment:
      _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS}
    depends_on:
      db:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully
    ports:
      # Host 8082 -> contenedor 8080
      - "8082:8080"
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    command: >
      airflow webserver

  # ======================
  # Airflow scheduler
  # ======================
  airflow-scheduler:
    image: apache/airflow:2.9.0-python3.10
    container_name: news_nlp_airflow_scheduler
    restart: unless-stopped
    env_file:
      - .env
    environment:
      _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS}
    depends_on:
      db:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    command: >
      airflow scheduler

volumes:
  news_nlp_db_data:
  mlflow_artifacts:
  airflow_logs:
